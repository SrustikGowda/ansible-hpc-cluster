# âš™ï¸ HPC Cluster Configuration with Ansible (SLURM)

A production-style Ansible playbook to configure an HPC cluster using SLURM workload manager.

---

## ğŸ”§ Features

- Role-based Ansible structure
- Installs SLURM, Munge, and dependencies
- Configures head and compute nodes
- Templated `slurm.conf` for flexibility

---

## ğŸ§± Folder Structure

```
ansible-hpc-cluster/
â”œâ”€â”€ inventories/
â”‚   â””â”€â”€ hosts
â”œâ”€â”€ site.yml
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â””â”€â”€ tasks/main.yml
â”‚   â””â”€â”€ slurm/
â”‚       â”œâ”€â”€ tasks/main.yml
â”‚       â””â”€â”€ templates/slurm.conf.j2
```

---

## ğŸš€ How to Run

1. Update IP addresses in `inventories/hosts`
2. Make sure all nodes are accessible via SSH
3. Run the playbook:
```bash
ansible-playbook -i inventories/hosts site.yml
```

---

## ğŸ“¦ Roles

- `common`: Base system packages and Munge
- `slurm`: SLURM controller and node setup

---

## ğŸ“š Useful Tips

- Requires Ubuntu or Debian-based system
- You can extend to support GPU nodes, multiple partitions, job submission testing

---

## ğŸ‘¨â€ğŸ’» Author

**Srustik**  
Senior DevOps Engineer | HPC Enthusiast  
ğŸ“§ Connect on [LinkedIn](https://www.linkedin.com)

---

## ğŸ“ƒ License

MIT License â€“ Use and adapt freely!